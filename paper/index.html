<!doctype html>
<html>
<head>
<title>neural audio: music information retrieval using deep neural networks</title>
<style type="text/css">
	body{
		height:36in;
		width:48in;
		font-family:Avenir;
		position:relative;
		font-size:38pt;
	}
	header{
		text-align:center;
		width:100%;
		height:4in;
	}
	header img{
		position:absolute;
		max-width:5.5in;
		top:.5in;
		height:auto;
	}
	header h1, header h2{
		margin-left:4.5in;
		margin-right:4.5in;
		font-weight:normal;
	}
	header h1{
		font-size:80pt;
	}
	header h2{
		font-size:60pt;
	}
	.leftlogo{
		left:1.15in;
		height:auto;
	}
	#LSUlogo{
		top:2in;
	}
	#LinfieldLogo{
		top:3in;
	}
	#NSFlogo{
		right:1.5in;
		top:0in;
		height:3.5in;
		width:3.5in;
	}

	h1{
		text-transform:capitalize;
	}
	section{
		width:15in;
		margin-left:.5in;
		margin-right:.5in;
		position:absolute;
		background-color:transparent !important;
		overflow:hidden;
	}
	section.first{
		left:.5in;
	}
	section.second{
		left:16in;
	}
	section.third{
		right:.5in;
	}
	p{
		margin-bottom:.1in;
		margin-top:.1in;
	}

	section h1{
		width:100%;
		text-align:center;
		font-size:60pt;
		font-weight:normal;
		margin-bottom:.25in;
		margin-top:.25in;
	}
	section h2{
		font-size:50pt;
		font-weight:normal;
		float:left;
		clear:left;
		margin-top:.25in;
		margin-right:.15in;
		padding-right:.15in;
		margin-bottom:.1in;
		border-right-width:5px;
		border-right-style:solid;
		background-color:transparent !important;
	}
	span.poster, span.neuralnet, span.genreID, span.instID, span.mir{
		background-color:transparent !important;
		color:inherit;
		border-bottom-style:solid;
		border-bottom-width:3px;
	}
	.poster{
		background-color:#babdb6;
		border-color:#babdb6;
	}
	.neuralnet{
		background-color:#73d216;
		border-color:#73d216;
	}
	.genreID{
		background-color:#729fcf;
		border-color:#729fcf;
	}
	.instID{
		background-color:#fcaf3e;
		border-color:#fcaf3e;
	}
	.mir{
		background-color:#ad7fa8;
		border-color:#ad7fa8;
		color:#eeeeec;
	}
	.wrong{ /* Putting this in here just so I have the red I used in the graph documented */
		background-color:#DE1515;
		border-color:#DE1515;
	}
	.code, a.code:link, a.code:visited, a.code:active{
		background-color:#888a85;
		color:#eeeeec;
		font-family:"Inconsolata", monospace;
		border-bottom-color:#babdb6;
	}
	.caption{
		font-size:20pt;
		margin-top:0;
		margin-bottom:0;
		text-align:center;
	}

	#introduction{
		height:15in;
	}
	#neurnet{
		top:15.5in;
		height:20in;
	}

	#instID{
		height:27in;
	}
	#references{
		top:28in;
		font-size:28pt;
	}
	img + p{
		margin-top:10px;
	}
	.beforeBlock{
		margin-bottom:0;
	}
	section img{
		margin-top:10px;
	}
	ul{
		margin-top:0;
	}
	img{
		float:left;
		width:15in;
		margin-bottom:.1in;
	}
</style>
</head>
<body>
<header>
	<img src="../data/cct.jpg" alt="Louisiana State University" id="LSUlogo" class="leftlogo">
	<img src="../data/linfield.gif" alt="Linfield College" id="LinfieldLogo" class="leftlogo">
	<h1>neural audio: <span class="mir">music information retrieval</span> using <span class="neuralnet">deep neural networks</span></h1>
	<h2>Grey Patterson (Linfield, CCT), Andrew Pfalz, and Jesse Allison (LSU School of Music/CCT)</h2>
	<img src="../data/nsf.png" alt="National Science Foundation" id="NSFlogo">
</header>
<section class="poster first" id="introduction">
	<h1 class="poster">introduction</h1>
	<img src="../data/qr.png" style="width:5in; margin-right:.25in">
	<p>Scan the QR code for the more verbose digital version of this poster.</p>
	<p>Music is rich in information - from things like what key and time signature are being played in up to the sociocultural context in which the lyrics were written.</p>
	<p>The field of music information retrieval exists to provide musicologists with automated tools. Complex tasks like <span class="genreID">genre recognition</span> remain out of reach. To solve this problem, we use neural networks: a form of <span class="neuralnet">machine learning</span> that consist of <span class="neuralnet">nodes</span> connected by <span class="neuralnet">weighted edges.</span> Input is propagated through the network from node to node along those edges.</p>
</section>
<section class="neuralnet first" id="neurnet">
	<h1 class="neuralnet">neural networks</h1>
	<div>
		<h2 class="neuralnet">structure</h2>
		<p>Neural networks are based on the structure of the human mind: nodes serve as neurons, and weighted lines between them alter the input values as they flow through the network.</p>
		<p>A <span class="neuralnet">deep neural network</span> is one with several <span class="neuralnet">hidden layers</span>. (Any layer other than the input and output layers is considered 'hidden.')</p>
		<img src="../data/NN.png" style="float:right; width:10in">
	</div>
	<div>
		
		<h2 class="neuralnet">training</h2>
		<p>Neural networks are an alternative to traditional programming. Instead of writing an algorithm yourself, you provide training data - inputs, and the output expected: <span class="code">([0, 821, 1643,...],"art")</span></p>
		<p>The software uses <span class="neuralnet">stochastic gradient descent</span> to minimize the <span class="neuralnet">cross-entropy</span> of the correct versus the calculated answers.</p>
	</div>
	<div>
		<h2 class="neuralnet">output</h2>
		<p>The output can be either a direct categorization or a <span class="neuralnet">softmax</span> (weighted probability distribution). We chose to use softmax throughout.</p>
	</div>
</section>
<section class="genreID second" id="genreID">
	<h1 class="genreID">genre identification</h1>
	<div>
		<h2 class="genreID">methods</h2>
		<p class="beforeBlock">We split input into <span class="genreID">three categories:</span> art, popular, and traditional. For reading in a song from the library, we used three <span class="mir">windows:</span></p>
		<img src="../data/figure_3.png" style="width:15in">
		<p>Each window of samples was fed into the neural network, which attempted to categorize it into one of the three genres; all inputs were annotated with 'correct' answers, used for training.</p>
	</div>
	<div>
		<h2 class="genreID">results</h2>
		<p>Overfitting proves to be a significant problem, but adjusting for it shows that randomizing samples within the song increases accuracy - though, at the same time, it decreased precision.</p>
		<img src="../data/Chart1.png" style="/*float:right; width:8in; margin-left:.25in*/">
		<img src="../data/Chart4.png" style="/*float:left; width:8in; margin-right:.25in*/">
	</div>
</section>
<section class="instID third" id="instID">
	<h1 class="instID">instrument identification</h1>
	<div>
		<h2 class="instID">methods</h2>
		<p>We broke song stems and mixes into four categories: vocal solo, guitar solo, drum solo, and ensemble 'other.'</p>
		<img src="../data/figure_2.png" style="float:right; width:10in;">
		<p>We broke each song into <span class="mir">windows</span> of 1-3 seconds, with .5-1.5 seconds of overlap. In training, four files (one from each category) were read in concurrently, randomizing the input to avoid overfitting. In evaluation, we fed an entire song through sequentially, yielding an array of categorization results. With that, we generated an array of start/stop times and their categories. The graph above is a visual representation of this result.</p>
	</div>
	
	<div>
		<h2 style="padding-bottom:.1in" class="instID">results</h2><p>We found that using raw samples, rather than FFT data, yielded higher overall accuracy.</p>
		<img src="../data/Chart5.png" style="/*float:left; clear:left; width:10in; margin-right:.25in*/">
		
	</div>
</section>
<section class="poster third" id="references">
	<h1 class="poster">references</h1>
	Chollet, F. <em>Keras</em>, 2015, https://github.com/fchollet/keras/<br />
	Li, T., Chan, A., Chun, A.. <em>"Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network"</em><br />
	R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, <em>"MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research"</em>, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014.<br />
	This material is based upon work supported by the National Science Foundation under award OCI-1560410 with additional support from the Center for Computation &amp; Technology at Louisiana State University.
</section>
</body>
</html>