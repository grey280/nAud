<!doctype html>
<html>
<head>
<title>neural audio: music information retrieval using deep neural networks</title>
<style type="text/css">
	body{
		height:36in;
		width:48in;
		font-family:Avenir;
		position:relative;
		font-size:38pt;
	}
	header{
		text-align:left;
		width:100%;
		height:4in;
	}
	header img{
		position:absolute;
		top:.5in;
		height:auto;
	}
	header h1, header h2{
		margin-left:1in;
		margin-right:4.5in;
		font-weight:normal;
	}
	header h1{
		font-size:80pt;
	}
	header h2{
		font-size:60pt;
	}
	.leftlogo{
		right:4.75in;
		width:8in;
		height:auto;
	}
	#LSUlogo{
		top:2.5in;
	}
	#LinfieldLogo{
		top:.5in;
		width:6in;
	}
	#NSFlogo{
		right:1in;
		top:0in;
		height:3.5in;
		width:3.5in;
	}

	h1{
		text-transform:capitalize;
	}
	section{
		width:15in;
		margin-left:.5in;
		margin-right:.5in;
		position:absolute;
		background-color:transparent !important;
		overflow:hidden;
	}
	section.first{
		left:.5in;
	}
	section.second{
		left:16in;
	}
	section.third{
		right:.5in;
	}
	p{
		margin-bottom:.1in;
		margin-top:.1in;
	}

	section h1{
		width:100%;
		text-align:center;
		font-size:60pt;
		font-weight:normal;
		margin-bottom:.25in;
		margin-top:.25in;
	}
	section h2{
		font-size:50pt;
		font-weight:normal;
		float:left;
		clear:left;
		margin-top:.25in;
		margin-right:.15in;
		padding-right:.15in;
		margin-bottom:.1in;
		border-right-width:5px;
		border-right-style:solid;
		background-color:transparent !important;
	}
	span.poster, span.neuralnet, span.genreID, span.instID, span.mir{
		background-color:transparent !important;
		color:inherit;
		border-bottom-style:solid;
		border-bottom-width:3px;
	}
	.poster{
		background-color:#babdb6;
		border-color:#babdb6;
	}
	.neuralnet{
		background-color:#73d216;
		border-color:#73d216;
	}
	.genreID{
		background-color:#729fcf;
		border-color:#729fcf;
	}
	.instID{
		background-color:#fcaf3e;
		border-color:#fcaf3e;
	}
	.mir{
		background-color:#ad7fa8;
		border-color:#ad7fa8;
		color:#eeeeec;
	}
	.wrong{ /* Putting this in here just so I have the red I used in the graph documented */
		background-color:#DE1515;
		border-color:#DE1515;
	}
	.code, a.code:link, a.code:visited, a.code:active{
		background-color:#888a85;
		color:#eeeeec;
		font-family:"Inconsolata", monospace;
		border-bottom-color:#babdb6;
	}
	.caption{
		font-size:20pt;
		margin-top:0;
		margin-bottom:0;
		text-align:center;
	}

	#introduction{
		height:15in;
	}
	#neurnet{
		top:15.5in;
		height:20in;
	}

	#instID{
		height:27in;
	}
	#references{
		top:28in;
		font-size:28pt;
	}
	img + p{
		margin-top:10px;
	}
	.beforeBlock{
		margin-bottom:0;
	}
	section img{
		margin-top:10px;
	}
	ul{
		margin-top:0;
	}
	img{
		float:left;
		width:15in;
		margin-bottom:.1in;
	}

	.oneline{
		padding-bottom:.65in;
	}
	.twoline{
		padding-bottom:1.4in;
	}
</style>
</head>
<body>
<header>
	<img src="../data/CCT.jpg" alt="Louisiana State University" id="LSUlogo" class="leftlogo">
	<img src="../data/linfield.gif" alt="Linfield College" id="LinfieldLogo" class="leftlogo">
	<h1>neural audio: <span class="mir">music information retrieval</span> using <span class="neuralnet">deep neural networks</span></h1>
	<h2>Grey Patterson (Linfield, CCT), Andrew Pfalz, and Jesse Allison (LSU School of Music/CCT)</h2>
	<img src="../data/NSF.png" alt="National Science Foundation" id="NSFlogo">
</header>
<section class="poster first" id="introduction">
	<h1 class="poster">introduction</h1>
	<img src="../data/qr.png" style="width:5in; margin-right:.25in">
	<p>Scan the QR code for the more verbose digital version of this poster.</p>
	<p>Music is rich in information - from things like what key and time signature are being played in up to the sociocultural context in which the lyrics were written.</p>
	<p>The field of music information retrieval exists to provide musicologists with automated tools. Complex tasks like <span class="genreID">genre recognition</span> remain out of reach. Neural networks offer a solution to this problem: a form of <span class="neuralnet">machine learning</span> that consist of <span class="neuralnet">nodes</span> connected by <span class="neuralnet">weighted edges.</span> Input is propagated through the network from node to node along those edges.</p>
</section>
<section class="neuralnet first" id="neurnet">
	<h1 class="neuralnet">neural networks</h1>
	<div>
		<h2 class="neuralnet">structure</h2>
		<p>Neural networks are based on the structure of the human mind: nodes serve as neurons, and weighted lines between them alter the input values as they flow through the network.</p>
		<p>A <span class="neuralnet">deep neural network</span> is one with several <span class="neuralnet">hidden layers</span>. (Any layer other than the input and output layers is considered 'hidden.')</p>
		
	</div>
	<div>
		<h2 class="neuralnet">training</h2>

		<p>Neural networks are an alternative to traditional programming.<img src="../data/NN.png" style="float:right; width:10in"> Instead of writing an algorithm by hand, simply provide training data - inputs, and the output expected: <span class="code">([0, 821, 1643, ...],"art")</span></p>

		<p>The software uses <span class="neuralnet">stochastic gradient descent</span> to minimize the <span class="neuralnet">cross-entropy</span> of the correct versus the calculated answers.</p>
	</div>
	<div>
		<h2 class="neuralnet twoline">output</h2>
		<p>The output can be either a direct categorization or a <span class="neuralnet">softmax</span> (weighted probability distribution). Softmax results were used throughout, as they provide more information for analysis.</p>
	</div>
</section>
<section class="genreID second" id="genreID">
	<h1 class="genreID">genre identification</h1>
	<div>
		<h2 class="genreID oneline">methods</h2>
		<p class="beforeBlock">Input was split into <span class="genreID">three categories:</span> art, popular, and traditional. For reading in a song from the library, one of three <span class="mir">windows</span> was used:</p>
		<img src="../data/figure_3.png" style="width:15in">
		<p>Each window of samples was fed into the neural network, which attempted to categorize it into one of the three genres; all inputs were annotated with 'correct' answers, used for training.</p>
	</div>
	<div>
		<img src="../data/Chart4.png" style="/*float:left; width:8in; margin-right:.25in*/">
		<h2 class="genreID twoline">results</h2>
		<p>Overfitting was a problem: below, most of the incorrect were misfiled into 'tradition.' Reading 1 minute into the song was most the precise, though not the most accurate.</p>
		<img src="../data/Chart1.png" style="/*float:right; width:8in; margin-left:.25in*/">
		
	</div>
</section>
<section class="instID third" id="instID">
	<h1 class="instID">instrument identification</h1>
	<div>
		<h2 class="instID oneline">methods</h2>
		<p>Song stems and mixes were broken into four categories: vocal solo, guitar solo, drum solo, and ensemble 'other.'</p>
		
		<p>Each song was broken into <span class="mir">windows</span> of 1-3 seconds, with .5-1.5 seconds of overlap. <img src="../data/figure_2.png" style="float:right; width:10in;">In training, four files (one from each category) were read in concurrently, shuffling the input to avoid overfitting. In evaluation, an entire song can be fed through sequentially, getting an array of categorization results. That was then converted into an array of category start/stop times. The graph above is a visual representation of this, showing the problem of overfitting.</p>
	</div>
	<div>
		<h2 style="padding-bottom:.1in" class="instID">results</h2><p>Using raw samples, rather than FFT data, appears to yield higher overall accuracy.</p>
		<img src="../data/Chart5.png" style="/*float:left; clear:left; width:10in; margin-right:.25in*/">
	</div>
</section>
<section class="poster third" id="references">
	<h1 class="poster">references</h1>
	Chollet, F. <em>Keras</em>, 2015, https://github.com/fchollet/keras/<br />
	Li, T., Chan, A., Chun, A.. <em>"Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network"</em><br />
	R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, <em>"MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research"</em>, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014.<br />
	This material is based upon work supported by the National Science Foundation under award OCI-1560410 with additional support from the Center for Computation &amp; Technology at Louisiana State University.
</section>
<!-- Piwik -->
<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(["setDomains", ["*.grey280.github.io"]]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//analytics.twoeighty.net/";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', 15]);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<noscript><p><img src="//analytics.twoeighty.net/piwik.php?idsite=15" style="border:0;" alt="" /></p></noscript>
<!-- End Piwik Code -->
</body>
</html>