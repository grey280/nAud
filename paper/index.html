<!doctype html>
<html>
<head>
<title>neural audio: music information retrieval using deep neural networks</title>
<style type="text/css">
	body{
		height:36in;
		width:48in;
		font-family:Avenir;
		position:relative;
		font-size:38pt;
	}
	header{
		text-align:center;
		width:100%;
		height:4in;
	}
	header img{
		position:absolute;
		max-height:3.5in;	
		max-width:3.5in;
		top:.5in;
	}
	header h1, header h2{
		margin-left:4.5in;
		margin-right:4.5in;
		font-weight:normal;
	}
	header h1{
		font-size:80pt;
	}
	header h2{
		font-size:60pt;
	}
	#LSUlogo{
		left:1.5in;
		top:1in;
	}
	#LinfieldLogo{
		left:1.45in;
		top:2.5in;
		height:.5in;
		max-width:3.53in;
	}
	#NSFlogo{
		right:1.5in;
		top:0in;
	}


	section{
		width:15in;
		margin-left:.5in;
		margin-right:.5in;
		position:absolute;
		background-color:transparent !important;
		overflow:hidden;
	}
	section.first{
		left:.5in;
	}
	section.second{
		left:16in;
	}
	section.third{
		right:.5in;
	}
	p{
		margin-bottom:.1in;
		margin-top:.1in;
	}

	section h1{
		width:100%;
		text-align:center;
		font-size:60pt;
		font-weight:normal;
		margin-bottom:.25in;
		margin-top:.25in;
	}
	section h2{
		font-size:50pt;
		font-weight:normal;
		float:left;
		clear:left;
		margin-top:.25in;
		margin-right:.15in;
		padding-right:.15in;
		margin-bottom:.1in;
		border-right-width:5px;
		border-right-style:solid;
		background-color:transparent !important;
	}
	span.poster, span.neuralnet, span.genreID, span.instID, span.mir{
		background-color:transparent !important;
		color:inherit;
		border-bottom-style:solid;
		border-bottom-width:3px;
	}
	.poster{
		background-color:#babdb6;
		border-color:#babdb6;
	}
	.neuralnet{
		background-color:#73d216;
		border-color:#73d216;
	}
	.genreID{
		background-color:#729fcf;
		border-color:#729fcf;
	}
	.instID{
		background-color:#fcaf3e;
		border-color:#fcaf3e;
	}
	.mir{
		background-color:#ad7fa8;
		border-color:#ad7fa8;
		color:#eeeeec;
	}
	.wrong{ /* Putting this in here just so I have the red I used in the graph documented */
		background-color:#DE1515;
		border-color:#DE1515;
	}
	.code, a.code:link, a.code:visited, a.code:active{
		background-color:#888a85;
		color:#eeeeec;
		font-family:"Inconsolata", monospace;
		border-bottom-color:#babdb6;
	}
	.caption{
		font-size:20pt;
		margin-top:0;
		margin-bottom:0;
		text-align:center;
	}

	#introduction{
		height:15in;
	}
	#neurnet{
		top:15.5in;
		height:20in;
	}

	#instID{
		height:29.5in;
	}
	#references{
		top:30in;
	}
	img + p{
		margin-top:10px;
	}
	.beforeBlock{
		margin-bottom:0;
	}
	section img{
		margin-top:10px;
	}
	ul{
		margin-top:0;
	}

	img{
		float:left;
		width:15in;
		margin-bottom:.1in;
	}

	#references ul{
		font-size:20pt;
	}
</style>
</head>
<body>
<!-- <div id="forceheight" style="position:absolute; height:5px; width:5px; top:34.5in; left:0px; background-color:red"></div> -->
<header>
	<img src="../data/lsu.png" alt="Louisiana State University" id="LSUlogo">
	<img src="../data/linfield.gif" alt="Linfield College" id="LinfieldLogo">
	<h1>neural audio: <span class="mir">music information retrieval</span> using <span class="neuralnet">deep neural networks</span></h1>
	<h2>Grey Patterson (Linfield, CCT), Andrew Pfalz, and Jesse Allison (LSU School of Music/CCT)</h2>
	<img src="../data/nsf.png" alt="National Science Foundation" id="NSFlogo">
</header>
<section class="poster first" id="introduction">
	<h1 class="poster">introduction</h1>
	<img src="../data/qr.png" style="width:5in; margin-right:.25in">
	<p>Scan the QR code for the more verbose digital version of this poster.</p>
	<p>Music is rich in information - from things like what key and time signature are being played in up to the sociocultural context in which the lyrics were written.</p>
	<p>The field of music information retrieval exists to provide musicologists with automated tools. Complex tasks like <span class="genreID">genre recognition</span> remain out of reach. To solve this problem, we use neural networks: a form of <span class="neuralnet">machine learning</span> that consist of <span class="neuralnet">nodes</span> connected by <span class="neuralnet">weighted edges.</span> Input is propagated through the network from node to node along those edges.</p>
</section>
<section class="neuralnet first" id="neurnet">
	<h1 class="neuralnet">neural networks</h1>
	<div>
		<h2 class="neuralnet">structure</h2>
		<p>Neural networks are based on the structure of the human mind: nodes serve as neurons, and weighted lines between them alter the input values as they flow through the network.</p>
		<p>A <span class="neuralnet">deep neural network</span> is one with several <span class="neuralnet">hidden layers</span>. (Any layer other than the input and output layers is considered 'hidden.')</p>
		<img src="../data/NN.png" style="float:right; width:10in">
	</div>
	<div>
		
		<h2 class="neuralnet">training</h2>
		<p>Neural networks are an alternative to traditional programming. Instead of writing an algorithm yourself, you provide training data - inputs, and the output expected: <span class="code">([0, 821, 1643,...],"art")</span></p>
		<p>The software uses <span class="neuralnet">stochastic gradient descent</span> to minimize the <span class="neuralnet">cross-entropy</span> of the correct versus the calculated answers.</p>
	</div>
	<div>
		<h2 class="neuralnet">output</h2>
		<p>The output can be either a direct categorization or a <span class="neuralnet">softmax</span> (weighted probability distribution). We chose to use softmax throughout.</p>
	</div>
</section>
<section class="genreID second" id="genreID">
	<h1 class="genreID">genre identification</h1>
	<div>
		<h2 class="genreID">methods</h2>
		<p class="beforeBlock">We split input into <span class="genreID">three categories:</span> art, popular, and traditional. For reading in a song from the library, we used three <span class="mir">windows:</span></p>
		<img src="../data/figure_3.png" style="width:15in">

		
		
	</div>
	<div>
		<h2 class="genreID">results</h2>
		
		<p>Overfitting proves to be a significant problem, but adjusting for it shows that randomizing samples within the song increases accuracy - though, at the same time, it decreased precision.</p>
		<img src="../data/Chart1.png" style="/*float:right; width:8in; margin-left:.25in*/">
		
		<img src="../data/Chart4.png" style="/*float:left; width:8in; margin-right:.25in*/">
		
	</div>
	<div>
		<h2 class="genreID">conclusion</h2>
		<p class="">Identifying which of the <span title="Art music, popular music, or traditional music." class="mir">three categories</span> a song falls into is a difficult task at times. Christmas music is a great example of how difficult this can be: <a class="citation" title="Pentatonix - Carol of the Bells (YouTube)">Pentatonix' cover of <em>Carol of the Bells</em></a> is a three-year-old version of a song that was penned in 1914, which was itself based on an even older Ukranian folk song. Is this popular or traditional music?</p>
	</div>
</section>
<section class="instID third" id="instID">
	<h1 class="instID">instrument identification</h1>
	<div>
		<h2 class="instID">methods</h2>
		<p>We broke song stems and mixes from MedleyDB (R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello) into four categories: vocal solo, guitar solo, drum solo, and ensemble 'other.'</p>
		<img src="../data/figure_2.png" style="float:right; width:12in;">
		<p>By breaking the songs into <span class="mir">samples</span> consisting of 1-3 seconds, with .5-1.5 seconds of overlap, we were able to process entire songs into an array of categorization results. With that, we generated an array of start/stop times and their categories.</p>
	</div>
	
	<div>
		<h2 style="padding-bottom:.1in" class="instID">results</h2>
		<img src="../data/Chart5.png" style="/*float:left; clear:left; width:10in; margin-right:.25in*/">
		<p>We found that using raw samples, rather than FFT data, yielded higher overall accuracy (ignoring the outlier).</p>
	</div>
	<div>
		<h2 class="instID">conclusion</h2>
		<p>Instrument identification remains an interesting problem, and one that we feel should be given further consideration. Increasing the size of the windows that the network was given yielded slight increases in accuracy, but we were unable to explore the extent to which this continues. Overall, we recommend further research with the aim of identifying which factors have the largest effect on accuracy.</p>
	</div>
</section>
<section class="poster third" id="references">
	<h1 class="poster">references</h1>
	<ul>
		<li>Chollet, F. <em>Keras</em>, 2015, https://github.com/fchollet/keras/</li>
		<li>Li, T., Chan, A., Chun, A.. <em>"Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network"</em></li>
		<li>R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, <em>"MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research"</em>, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014.</li>
		<li>Work supported by the National Science Foundation, award OCI-1560410. Additional support from the Center for Computation &amp; Technology at LSU.</li>
	</ul>
</section>
</body>
</html>