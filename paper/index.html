<!doctype html>
<html>
<head>
<title>neural audio: music information retrieval using deep neural networks</title>
<style type="text/css">
	body{
		height:36in;
		width:48in;
		font-family:Avenir;
		position:relative;
		font-size:30pt;
	}
	header{
		text-align:center;
		width:100%;
		height:4in;
	}
	header img{
		position:absolute;
		max-height:3.5in;	
		max-width:3.5in;
		top:.5in;
	}
	header h1, header h2{
		margin-left:4.5in;
		margin-right:4.5in;
		font-weight:normal;
	}
	header h1{
		font-size:80pt;
	}
	header h2{
		font-size:60pt;
	}
	#LSUlogo{
		left:1.5in;
		top:1.15in;
	}
	#NSFlogo{
		right:1.5in;
		top:0in;
	}
	section{
		width:15in;
		margin-left:.5in;
		margin-right:.5in;
		position:absolute;
		background-color:transparent !important;
		overflow:hidden;
	}
	section.first{
		left:.5in;
	}
	section.second{
		left:16in;
	}
	section.third{
		right:.5in;
	}

	section h1{
		width:100%;
		text-align:center;
		font-size:60pt;
		font-weight:normal;
		margin-bottom:.25in;
		margin-top:.25in;
	}
	section h2{
		font-size:50pt;
		font-weight:normal;
		float:left;
		clear:left;
		margin-top:0;
		margin-right:.15in;
		padding-right:.15in;
		margin-bottom:.1in;
		border-right-width:5px;
		border-right-style:solid;
		background-color:transparent !important;
	}
	span.poster, span.neuralnet, span.genreID, span.instID, span.mir{
		background-color:transparent !important;
		color:inherit;
		border-bottom-style:solid;
		border-bottom-width:3px;
	}
	.poster{
		background-color:#babdb6;
		border-color:#babdb6;
	}
	.neuralnet{
		background-color:#73d216;
		border-color:#73d216;
	}
	.genreID{
		background-color:#729fcf;
		border-color:#729fcf;
	}
	.instID{
		background-color:#fcaf3e;
		border-color:#fcaf3e;
	}
	.mir{
		background-color:#ad7fa8;
		border-color:#ad7fa8;
		color:#eeeeec;
	}
	.wrong{ /* Putting this in here just so I have the red I used in the graph documented */
		background-color:#DE1515;
		border-color:#DE1515;
	}
	.code, a.code:link, a.code:visited, a.code:active{
		background-color:#888a85;
		color:#eeeeec;
		font-family:"Inconsolata", monospace;
		border-bottom-color:#babdb6;
	}
	.caption{
		font-size:20pt;
		margin-top:0;
		margin-bottom:0;
		text-align:center;
	}

	#introduction{
		height:15in;
	}
	#neurnet{
		top:15.5in;
		height:20in;
	}

	#instID{
		height:28in;
	}
	#references{
		top:28.5in;
	}
	img + p{
		margin-top:10px;
	}
	.beforeBlock{
		margin-bottom:0;
	}
	section img{
		margin-top:10px;
	}
	ul{
		margin-top:0;
	}
</style>
</head>
<body>
<!-- <div id="forceheight" style="position:absolute; height:5px; width:5px; top:34.5in; left:0px; background-color:red"></div> -->
<header>
	<img src="lsu.png" alt="Louisiana State University" id="LSUlogo">
	<h1>neural audio: <span class="mir">music information retrieval</span> using <span class="neuralnet">deep neural networks</span></h1>
	<h2>Grey Patterson (Linfield, CCT), Andrew Pfalz, and Jesse Allison (LSU School of Music/CCT)</h2>
	<img src="nsf.png" alt="National Science Foundation" id="NSFlogo">
</header>
<section class="poster first" id="introduction">
	<h1 class="poster">introduction</h1>
	<img src="../data/qr.png" style="float:left; width:6in; margin-right:.25in">
	<p>Scan the QR code for the more verbose digital version of this poster.</p>
	<p>Neural networks are a form of <span class="neuralnet">machine learning</span> that consist of <span class="neuralnet">nodes</span> connected by <span class="neuralnet">weighted edges.</span> Input goes in as floating-point numbers, and is propagated through the network from node to node along those edges.</p>
	<p>Music is rich in information - from things like what key and time signature are being played in up to the sociocultural context in which the lyrics were written.</p>
	<p>The field of music information retrieval exists to provide musicologists with automated tools. Complex tasks, though, remain out of reach. There is no algorithm to identify the <span class="genreID">genre</span> of a piece, and no machine can identify which <span class="instID">instrument</span> is playing as easily as a human can.</p>
</section>
<section class="neuralnet first" id="neurnet">
	<h1 class="neuralnet">neural networks</h1>
	<div>
		<h2 class="neuralnet">input</h2>
		<p>Neural networks are algorithms: they have inputs and outputs, and do something to the input to yield the output. For audio processing, this input can be either <span class="mir">audio samples</span> or <span class="mir">Fast Fourier Transform data</span>. We elected to use raw audio samples throughout.</p>
		<p>The inputs are <span class="mir">windows</span> of a fixed length: a set number of samples being fed in.</p>
	</div>
	<div>
		<h2 class="neuralnet">training</h2>
		<p>Neural networks are an alternative to traditional programming. Instead of writing an algorithm yourself, you provide training data - inputs, and the output expected: <span class="code">([0, 821, 1643,...],"art")</span></p>
		<p>The neural network itself is a series of <span class="neuralnet">layers</span>, each one consisting of one or more <span class="neuralnet">neurons</span>. Each neuron can have many inputs, and a single output that can be sent along to many other neurons. As a number moves from one neuron to another, it is altered by the <span class="neuralnet">weight</span> of that neuron.</p>
		<p>The software takes the inputs, runs them through a network with randomly-generated weights, then changes all the weights a bit and tries again.If it is closer to the right answer, it will keep changing the weights in that direction; if it got worse, it will go in a different direction. This is <span class="neuralnet">Stochastic Gradient Descent</span>.</p>
	</div>
	<div>
		<h2 class="neuralnet">output</h2>
		<p>The output of the <span class="neuralnet">neural network</span> can take a few different forms, depending on what is desired.</p>
		<p>There is the <span class="neuralnet">categorization</span>, which will taken an input and return the number of the category. Alternately, a <span class="neuralnet">softmax</span> layer will return the neural network's probabilities for <em>all</em> of the categories.</p>
		<p>Both give the same result - it is probably category 2 - but with the softmax layer it is visible how sure of the decision the neural network is. Because of this additional information, we used softmax outputs throughout.</p>
	</div>
</section>
<section class="genreID second" id="genreID">
	<h1 class="genreID">genre identification</h1>
	<div>
		<h2 class="genreID">methods</h2>
		<p>Based on <a class="citation" >"Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network" (Li, T., Chan, A., Chun, A..)</a>, we wanted to split music into at most four categories. For this, we opted to use Philip Tagg's axiomatic triangle, giving us <span class="genreID">three categories:</span> art, popular, or traditional.</p>
		<p>Our dataset was composed of the iTunes library of a member of the research team, but was later expanded through the use of open-domain recordings of 'art' and 'traditional' music. 10-25% of the data was used as <span class="neuralnet">validation data.</span></p>
		<p class="beforeBlock">Three <span class="mir">windows</span> were used:</p>
		<img src="../data/figure_3.png" style="width:15in">
		<p>A variety of neural network structures were attempted, with the resulting models and weights stored and the training-reported accuracies logged.</p>
	</div>
	<div>
		<h2 class="genreID">results</h2>
		
		<p>Overfitting proves to be a significant problem - the graph at right shows that identification of 'art' and 'tradition' failed nearly all of the time, when trained against a biased dataset.</p>
		<img src="../data/Chart1.png" style="float:right; width:8in; margin-left:.25in">
		<p class="beforeBlock">Alleviating this by limiting the size of the sample set allows other trends to come to light. Most notable of these is a trend towards <span class="genreID">more accuracy when using randomized samples</span> from within the songs, as opposed to samples from a fixed point. Shuffling these samples while training, to create an increase in the amount of coverage of any given song, increased accuracy from 5-20 percentage points.</p>
		<img src="../data/Chart4.png" style="float:left; width:8in; margin-right:.25in">
		<p>Even within the biased sample set, using these randomized samples increased accuracy, more than any change to the structure of the neural network did. Comparing the three windows mentioned above showed a minimum 5 percentage point increase in accuracy.</p>
	</div>
	<div>
		<h2 class="genreID">conclusion</h2>
		<p class="">Identifying which of the <span title="Art music, popular music, or traditional music." class="mir">three categories</span> a song falls into is a difficult task at times. Christmas music is a great example of how difficult this can be: <a class="citation" title="Pentatonix - Carol of the Bells (YouTube)">Pentatonix' cover of <em>Carol of the Bells</em></a> is a three-year-old version of a song that was penned in 1914, which was itself based on an even older Ukranian folk song. Is this popular or traditional music?</p>
		<p>Another source of error could be the nature of music - popular music is based upon what came before it, and much research has gone into the different ways in which aspects of traditional and art music can be identified in popular music.</p>
	</div>
</section>
<section class="instID third" id="instID">
	<h1 class="instID">instrument identification</h1>
	<div>
		<h2 class="instID">methods</h2>
		<p>We broke song stems and mixes from MedleyDB (R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello) into four categories: vocal solo, guitar solo, drum solo, and non-solo 'other.'</p>
		<img src="../data/figure_2.png" style="float:right;">
		<p>By breaking the songs into <span class="mir">samples</span> consisting of 1-3 seconds, with .5-1.5 seconds of overlap, we were able to process entire songs into an array of categorization results. With that, we generated an array of start/stop times and their categories, which can then be passed through a visualization utility to produce graphs as above.</p>
	</div>
	
	<div>
		<h2 style="padding-bottom:.1in" class="instID">results</h2>
		<img src="../data/Chart5.png" style="float:left; clear:left; width:10in; margin-right:.25in">
		<p>The highest single-test accuracy was produced by running the input through a <span class="mir">Fast Fourier Transform</span> prior to feeding it into the neural network; however, separate testing of that one revealed it to be not nearly as accurate, and we suspect there was an error in the input that allowed it to overfit to the sound of a guitar playing.</p>
		<p>Feeding the networks raw samples, without the intermediate step of an FFT, yielded higher average accuracy in detection, routinely outperforming the 'random guess' level of accuracy that the FFT tests produced.</p>
	</div>
	<div>
		<h2 class="instID">conclusion</h2>
		<p>Instrument identification remains an interesting problem, and one that we feel should be given further consideration. Increasing the size of the windows that the network was given yielded slight increases in accuracy, but we were unable to explore the extent to which this continues. Overall, we recommend further research with the aim of identifying which factors have the largest effect on accuracy.</p>
	</div>
</section>
<section class="poster third" id="references">
	<h1 class="poster">references</h1>
	<ul>
		<li>Chollet, F. <em>Keras</em>, 2015, https://github.com/fchollet/keras/</li>
		<li>Li, T., Chan, A., Chun, A.. <em>"Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network"</em></li>
		<li>R. Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam and J. P. Bello, <em>"MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research"</em>, in 15th International Society for Music Information Retrieval Conference, Taipei, Taiwan, Oct. 2014.</li>
	</ul>
</section>
</body>
</html>