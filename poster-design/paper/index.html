<!doctype html>
<html>
<head>
<title>neural audio: music information retrieval using deep neural networks</title>
<style type="text/css">
body{
	height:36in;
	width:48in;
	font-family:Avenir;
	position:relative;
	font-size:30pt;
}
header{
	text-align:center;
	width:100%;
	height:4in;
}
header img{
	position:absolute;
	max-height:3.5in;	
	max-width:3.5in;
	top:.5in;
}
header h1, header h2{
	margin-left:4.5in;
	margin-right:4.5in;
	font-weight:normal;
}
header h1{
	font-size:80pt;
}
header h2{
	font-size:60pt;
}
#LSUlogo{
	left:.5in;
}
#NSFlogo{
	right:.5in;
	top:0in;
}
section{
	width:15in;
	margin-left:.5in;
	margin-right:.5in;
	position:absolute;
	background-color:transparent !important;
}
section.first{
	left:.5in;
}
section.second{
	left:16in;
}
section.third{
	right:.5in;
}

section h1{
	font-size:60pt;
	font-weight:normal;
}
section h2{
	font-size:50pt;
	font-weight:normal;
	float:left;
	margin-top:0;
	margin-right:.1in;
}

.neuralnet{
	background-color:#8ae234;
}
.genreID{
	background-color:#729fcf;
}
.instID{
	background-color:#fcaf3e;
}
.mir{
	background-color:#ad7fa8;
	color:#eeeeec;
}
section h1{
	width:100%;
	text-align:center;
}

#introduction{
	height:15in;
}
#neurnet{
	top:16in;
	height:20in;

}
</style>
</head>
<body>
<header>
	<img src="lsu.png" alt="Louisiana State University" id="LSUlogo">
	<h1>neural audio: music information retrieval using deep neural networks</h1>
	<h2>Grey Patterson (Linfield), Andrew Pfalz, Dr. Jesse Allison (LSU CCT)</h2>
	<img src="nsf.png" alt="National Science Foundation" id="NSFlogo">
</header>
<section class="poster first" id="introduction">
	<h1>introduction</h1>
	<div>
		<p>Neural networks are a form of <span class="neuralnet">machine learning</span> based off the functionality of the human brain. They consist of <span class="neuralnet">nodes</span> connected by <span class="neuralnet">weighted edges</span> - similar to the structure of neurons. Input goes in as floating-point numbers, and is propagated through the network from node to node along those edges.</p>
		<p>The outputs and inputs can be basically anything you want: all of the image-recognition features in Google and Apple Photos are powered by neural networks; Microsoft's Skype Translate feature uses neural networks to perform real-time translation; the DeepMind group used neural networks to create AlphaGo, the world's best Go player.</p>
	</div>
	<div>
		<p>To date, however, there has been very little use of this form of machine learning in the field of music information retrieval. Services like Pandora and Spotify provide algorithmic music detection, but their actual databases are assembled through human labor and data mining, respectively.</p>
		<p>Music is rich in information - from things like what key and time signature are being played in up to the sociocultural context in which the lyrics were written. Musicologists work to extract this information in meaningful ways, but there is far more music in the world than there are musicologists. Providing automated tools to ease their efforts could be of significant benefit.</p>
		<p>The field of <span class="mir">music information retrieval</span> exists to provide these tools, and they have: automated beat-detection is doable, and identifying the key can be done with some effort. More complex tasks, though, remain out of reach. There is no algorithm to identify the <span class="genreID">genre</span> of a piece, and no machine can identify which <span class="instID">instrument</span> is playing as easily as a human can.</p>
	</div>
</section>
<section class="neuralnet first" id="neurnet">
	<h1>neural networks</h1>
	<div>
		<h2>input</h2>
		<p><span class="neuralnet">Neural networks</span> are basically algorithms: they have inputs and outputs, and do something to the input to yield the output.</p>
		<p>For audio processing, this input can be either <span class="mir">audio samples</span> or <span class="mir">Fast Fourier Transform data</span>. We elected to use raw audio samples throughout.</p>
		<p>The inputs, then, are <span class="mir">windows</span> of a fixed length, meaning that a fixed number of samples are being fed in.</p>
		<p>The samples are read in from .wav files by <span class="code">scipy.io.wavfile</span>, and look something like this:</p>
		<p><span class="code">0 821  1643  2461  3278  4092 4901 5705 6507 7294 8086 8859 9630 10389 11137 11876 12605 13314 14021 14702 15380 16036 16678 17304 17912 18502 19078 19629 20167 20679 21174 21651 22099 22535 22939 23329 23689 24033 24345 24642 24907 25152 25372 25564 25736 25878 25996 26092 26153 26202 26212 26206 26168 26106 26022 25905 25774 25600 25423 25201 24967 24703 24414 24108 23767 23416 23026 22632 ...</span></p>
		<p>And so on for quite a while - there's 44,100 of those numbers per second.</p>
		<p>That's a <em>lot</em> of numbers, which is where the windows come in - by picking one window, processing it, then moving forward a bit, using that as your new window, processing that, and then repeating, you can process a lot of audio.</p>
	</div>
	<div>
		<h2>training</h2>
		<p><span class="neuralnet">Neural networks</span> are an alternative to traditional programming.</p>
		<p>Instead of writing an algorithm yourself, you provide training data - inputs, and the output you're expecting.</p>
		<p>This looks something like this: <span class="code">([0, 821, 1643, 2461, 3278, 4092, 4901,...],"art")</span></p>
		<p>The neural network itself is a series of <span class="neuralnet">layers</span>, each one consisting of one or more <span class="neuralnet">neurons</span>. Each neuron can have many inputs, and a single output that can be sent along to many other neurons.</p>
		<p>What's important is the way the numbers move around - as a number moves from one neuron to another, it is altered by the <span class="neuralnet">weight</span> of that neuron.</p>
		<p>The neural network software - in this case, the <a target="_blank" href="http://keras.io" class="code citation">Keras</a> library running on either the <span class="code">Theano</span> or <span class="code">TensorFlow</span> backend - takes the inputs and runs them through a neural network with randomly-generated weights.</p>
		<p>Then the software changes all of the weights a bit and tries again. If it's closer to the right answer, it'll keep changing the weights in that direction; if it got worse, it'll go in a different direction. This is known as <span class="neuralnet">Stochastic Gradient Descent</span>.</p>
	</div>
	<div>
		<h2>output</h2>
		<p>The output of the <span class="neuralnet">neural network</span> can take a few different forms, depending on what you want.</p>
		<p>There's the <span class="neuralnet">categorization</span>, which will taken an input and give you the number of the category it's in, so <span class="code">nn.evaluate([0, 821, 1643, 2461, 3278, 4092, 4901,...])</span> would return <span class="code">2</span>, where 2 is the category ID for 'art.'</p>
		<p>Or you can use a <span class="neuralnet">softmax</span> layer, which will instead give you the neural network's probabilities for <em>all</em> of the categories.</p>
		<p><span class="code">nn.evaluate([0, 821, 1643, 2461, 3278, 4092, 4901,...])</span> would then give you <span class="code">[0.05, 0.11, 0.75, 0.08]</span>. Both tell you the same thing - it's probably category 2 - but with the softmax layer you can see how sure of the decision the neural network is.</p>
		<p>Because of this additional information, we used softmax outputs throughout.</p>
	</div>
</section>
<section class="genreid second">
	<h1>genre identification</h1>
</section>
<section class="instid third">
	<h1>instrument identification</h1>
</section>
</body>
</html>